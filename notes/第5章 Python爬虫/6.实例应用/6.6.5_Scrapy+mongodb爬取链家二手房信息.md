# Scrapy+mongodb爬取链家二手房信息

---

```python
# 创建项目
scrapy startproject room
cd room
scrapy genspider lianjia lianjia.com
```

```python
# settings
from fake_useragent import UserAgent

USER_AGENT = UserAgent().random
ROBOTSTXT_OBEY = False
DOWNLOAD_DELAY = 1
ITEM_PIPELINES = {
   'room.pipelines.MongoPipeline': 300,
}
LOG_LEVEL = 'ERROR'
```

```python
# start
from scrapy.cmdline import execute

execute('scrapy crawl lianjia'.split())
```

```python
# spider
import scrapy

class LianjiaSpider(scrapy.Spider):
    name = 'lianjia'
    # allowed_domains = ['lianjia.com']
    start_urls = ['https://sh.lianjia.com/ershoufang/pg{}/'.format(i) for i in range(1, 3)]

    def parse(self, response):
        urls = response.xpath('//div[@class="info clear"]/div[@class="title"]/a/@href').extract()
        for url in urls:
            yield scrapy.Request(url, callback=self.parse_info)

    def parse_info(self, response):
        total = response.xpath('concat(//span[@class="total"]/text(),//span[@class="unit"]/span/text())').extract_first()
        unit_price = response.xpath('string(//span[@class="unitPriceValue"])').extract_first()
        mian_ji = response.xpath('//div[@class="area"]/div[1]/text()').extract_first()
        fang_xin = response.xpath('//div[@class="room"]/div[1]/text()').extract_first()
        yield {
            'total': total,
            'unit_price': unit_price,
            'mian_ji': mian_ji,
            'fang_xin': fang_xin
        }
```

```python
# pipelines
import pymongo

class MongoPipeline(object):
    def open_spider(self, spider):
        self.client = pymongo.MongoClient()

    def process_item(self, item, spider):
        self.client.room.lianjia.insert(item)
        return item

    def close_spider(self, spider):
        self.client.close()
```

