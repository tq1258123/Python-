# Scrapy框架的使用

---

Pyspider框架可以快速完成爬虫的编写，但是也有一些缺点，比如可配置化程度不高，异常处理能力有限等。它对于一些反爬程度非常强的网站的爬取显得力不从心。

Scrapy功能非常强大，爬取效率高。相关扩展组件多，可配置和可扩展程度非常高，它几乎可以应对多有反爬网站，是目前Python中使用最广泛的爬虫框架。

## Scrapy框架介绍

Scrapy是一个基于Twisted的异步处理框架，是纯Python实现的爬虫框架，其架构清晰，模块之间的耦合程度低，可扩展性强，可以灵活完成各种需求。

### 架构介绍

![scrapy框架](D:\repository\PythonNotes\images\scrapy框架.png)

Engine，引擎，处理整个系统的数据流处理、出发事务，是整个框架的核心。

Item，项目，它定义了爬取结果的数据结构，爬取的数据会被赋值成该Item对象。

Scheduler，调度器，接收引擎发过来的请求并将其加入队列中，在引擎再次请求的时候将请求提供给引擎。

Downloader，下载器，下载网页内容，并将网页内容返回给蜘蛛。

Spider，蜘蛛，其内定义了爬取的逻辑和网页的解析规则，它主要负责解析响应并生成提取结果和存储数据。

Downloader Middlewares，下载器中间件，位于引擎和下载器之间的钩子框架，主要处理引擎与下载器之间的请求及响应。

Spider Middlewares，蜘蛛中间件，位于引擎和蜘蛛之间的钩子框架，主要处理蜘蛛输入的响应和输出的结果及新的请求。

### 数据流

1. Engine首先打开一个网站，找到处理该网站的Spider，并向该Spider请求第一个要爬取的URL。
2. Engine从Spider中获取到第一个要爬取的URL，并通过Scheduler以Request的形式调度。
3. Engine向Scheduler请求下一个要爬取的URL。
4. Scheduler返回下一个要爬取的URL给Engine，Engine将URL通过Downloader Middlewares转发给Downloader下载。
5. 一旦页面下载完毕，Downloader生成该页面的Response，并将其通过Downloader Middlewares发送给Engine。
6. Engine从下载器中接收到Response，并将其通过Spider Middlewares发送给Spider处理。
7. Spider处理Response，并返回提取到的Item及新的Request给Engine。
8. Engine将Spider返回的Item给Item Pipeline，将新的Request给Scheduler。
9. 重复第二步到第八步，知道Scheduler中没有更多的Request，Engine关闭该网站，爬取结束。

## Scrapy入门

![scrapy设置](D:\repository\PythonNotes\images\scrapy设置.png)

### 创建项目

在创建Scrapy项目时，需要使用pycharm里面的terminal终端，先定位到指定文件夹，然后再创建

```python
scrapy startproject wldf
cd wldf
scrapy genspider wldf_spier www.xxx.com
```

### 文件讲解

#### main

main文件主要是用来简化Scrapy代码的执行

```python
from scrapy.cmdline import excute   
excute(['scrapy','crawl','name'])
```

#### settings

settings文件初始化设置框架，主要需要修改的地方有

1. `User-Agent`

2. `ROBOTSTXT_OBEY = False`不遵守robots协议

3. `DOWNLOAD_DELAY = 2`下载延迟

4. ```python
   # 保存数据
   ITEM_PIPELINES = {
      'wldf.pipelines.WldfPipeline': 300,
   }
   ```

5. `LOG_LEVEL = 'ERROR'`设置日志级别

### 爬取小说案例——下载后小说的格式有待改进

首先根据前面介绍的内容设置settings，然后在进行爬取操作，有改变的部分在下面。

```python
# wldf_spider
import scrapy


class WldfSpiderSpider(scrapy.Spider):
    name = 'wldf_spider'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['http://www.xbiquge.la/0/10/7109.html']

    def parse(self, response):
        title = response.xpath('//h1/text()').extract_first()
        content = response.xpath('//div[@id="content"]/text()').extract()
        content = '\n'.join(content)
        print(title)
        yield {
            'title': title,
            'content': content
        }
        next_url = response.xpath('//div[@class="bottem1"]/a[4]/@href').extract_first()
        # 判断是否最后一页
        if next_url.find('.html') != -1:
            # 循环爬取，自动拼接链接
            yield scrapy.Request(response.urljoin(next_url), callback=self.parse)
```

```python
# pipelines
# 避免重复打开文件，所有先写一个打开文件函数，最后再写一个关闭文件函数
class WldfPipeline(object):
    def open_spider(self, spider):
        self.file = open('wldf.txt', 'a', encoding='utf-8')

    def process_item(self, item, spider):
        title = item['title']
        content = item['content'].replace('    ','')
        info = title + '\n' + content + '\n'
        self.file.write(info)
        return item

    def close_spider(self, spider):
        self.file.close()
```

```python
# main
from scrapy.cmdline import execute

execute(['scrapy', 'crawl', 'wldf_spider'])
```

## CrawlSpider

Crawlspider是Spider的派生类(一个子类)，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取的工作更适合。

![CrawlSpider原理图](D:\repository\PythonNotes\images\CrawlSpider原理图.png)

### 实例演示

由于其他的代码和Scrapy一样，这里只放爬虫部分代码

```python
# 新建项目
scrapy startproject wldf
cd wldf
scrapy genspider -t crawl wldf_spier www.xxx.com
```

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class ZwwSpider(CrawlSpider):
    name = 'zww'
    # allowed_domains = ['81zw.cn']
    start_urls = ['https://www.zwdu.com/book/8725/']

    rules = (
        # 匹配第一页,其实页面要更改
        Rule(LinkExtractor(restrict_xpaths=r'//dl/dd[2]/a'), callback='parse_item', follow=True),
        Rule(LinkExtractor(restrict_xpaths=r'//div[@class="bottem1"]/a[3]/@href'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        title = response.xpath('//h1/text()').extract_first()
        content = ''.join(response.xpath('//div[@id="content"]/text()').extract())
        yield {
            'title': title,
            'content': content
        }
```

##  ImagesPipeline

这个管道，提供了一个方便并具有额外特性的方法，来下载并本地保存图片。

- 将所有下载的图片转换成通用的格式`jpg/rgb`
- 避免重新下载最近已经下载过的图片
- 可以生产缩略图
- 检测图像的宽高，确保他们满足最小限制

这个管道也会为哪些当前安排好要下载的图片保留一个内部队列，并将那些到达的包含相同图片的项目连接到那个队列中。这可以避免多次下载几个项目共享的同一个图片。

### 实例爬取图片

这里有问题，图片的网页没有下一张图片的链接，采取分页网站爬取，但是结果只有一张照片

```python
# zol爬虫文件
import scrapy

class ZolSpider(scrapy.Spider):
    name = 'zol'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['http://bbs.zol.com.cn/dcbbs/gallery_d16_460265.html#p1']

    def parse(self, response):
        image_url = response.xpath('//div[@class="big-pic-box loading"]/img/@src').extract()
        image_name = response.xpath('string(//h3)').extract_first()
        yield {
            'image_urls': image_url,
            'image_name': image_name
        }
        for i in range(2, 10):
            url = 'http://bbs.zol.com.cn/dcbbs/gallery_d16_460265.html#p{}'.format(i)
            yield scrapy.Request(url=url, callback=self.parse)
```

```python
# settings
from fake_useragent import UserAgent

USER_AGENT = UserAgent().chrome
ROBOTSTXT_OBEY = False
ITEM_PIPELINES = {
    'tupian.pipelines.ImagePipeline': 300
    # 'scrapy.pipelines.images.ImagesPipeline': 300
}
IMAGES_STORE = r'D:\自我学习\python\Python练习\爬虫框架\tupian\tupian\spiders\tupian'
LOG_LEVEL = 'ERROR'
```

```python
# pipelinse
from scrapy.pipelines.images import ImagesPipeline
import scrapy

class TupianPipeline(object):
    def process_item(self, item, spider):
        return item

class ImagePipeline(ImagesPipeline):
    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            yield scrapy.Request(image_url, meta={"image_name": item["image_name"]})

    def file_path(self, request, response=None, info=None):
        file_name = request.meta['image_name'].strip() + '.jpg'
        file_name = file_name.replace('/', '_')
        return file_name
```

```python
# main
from scrapy.cmdline import execute

execute('scrapy crawl zol'.split())
```

## 设置动态UA

```python
# middlewares
from fake_useragent import UserAgent

class UserAgentDownloadMiddleware(object):
    def process_request(self, request, spider):
        request.headers.setdefault(b'User-Agent', UserAgent.random)
```

```python
# settings
DOWNLOADER_MIDDLEWARES = {
   'tupian.middlewares.UserAgentDownloadMiddleware(object):': 543,
}
```

## 代理设置

```python
# middlewares
class ProxyMiddleware(object):
    def process_request(self, request, spider):
        # request.meta['proxy'] = 'http://name:password@ip:port'
        request.meta['proxy'] = 'http://ip:port'
```

```python
# settings
DOWNLOADER_MIDDLEWARES = {
   'tupian.middlewares.ProxyMiddleware(object):': 543,
}
```

## 模拟登陆

```python
# 方式一
# 维持会话，不需要再验证
import scrapy

class Login1Spider(scrapy.Spider):
    name = 'login1'
    # allowed_domains = ['sxt.cn']
    # start_urls = ['http://sxt.cn/']
    def start_requests(self):
        url = 'http://www.sxt.cn/index/login/login.html'
        form_data = {
            'user':'17703181473',
            'password':'123456'
        }
        yield scrapy.FormRequest(url, formdata=form_data, callback=self.parse)

    def parse(self, response):
        yield scrapy.Request('http://www.sxt.cn/index/user.html', callback=self.parse_info)

    def parse_info(self, response):
        print(response.text)
```

## Scrapy保存到数据库

```python
# pipelines
import pymongo

class Login2Pipeline(object):
    def open_spider(self, spider):
        self.client = pymongo.MongoClient()

    def process_item(self, item, spider):
        self.client.douban.movie.insert_one(item)
        return item

    def close_spider(self, spider):
        self.client.close()
```

## Scrapy调试

用命令在终端进行调试：`scrapy shell 网址`，通过response查找需要的内容是否可以正确获得。

## Scrapy解除去重

```python
# spider
def start_requests(self):
    yield scrapy.Request('http://www.baidu.com', dont_filter=True, callback=self.parse)
```

## Pipeline功能

```python
# 当有多个pipeline，去除已经在前面保存的数据，后面的管道将不接受之前已经保存的数据
from scrapy.exceptions import DropItem

class Login2Pipeline(object):
    def process_item(self, item, spider):
        if item['type'] == '动作':
            sql.insert(item)
        else:
            raise DropItem
        return item
```

## Scrapy与Splash结合

```python
# settings
SPLASH_URL = 'http://192.168.99.100:8050/'
DOWNLOADER_MIDDLEWARES = {
   'scrapy_splash.SplashCookiesMiddleware': 723,
   'scrapy_splash.SplashMiddleware': 725,
   'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}
SPIDER_MIDDLEWARES = {
   'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,
}
# 去重
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'
```

```python
# spider第一种方式
import scrapy
from scrapy_splash import SplashRequest

class LoginSpider(scrapy.Spider):
    name = 'login'
    def start_requests(self):
        url = 'http://www.baidu.com'
        yield SplashRequest(url, callback=self.parse(), args={'wait':1})

    def parse(self, response):
        pass
```

```python
# spider第二种方式
import scrapy
from scrapy_splash import SplashRequest

class LoginSpider(scrapy.Spider):
    name = 'login'
    def start_requests(self):
        url = 'http://www.baidu.com'
        lua = """
            function main(splash, args)
              assert(splash:go(args.url))
              assert(splash:wait(1))
              return splash:html()
            end
        """
        yield SplashRequest(url, callback=self.parse(), endpoint='execute', args={'lua_source': lua})

    def parse(self, response):
        pass
```

## Scrapy与Selenium结合

```python
# middleware
from selenium import webdriver
from scrapy.http import HtmlResponse

class SeleniumMiddleware(object):
    def process_request(self, request, spider):
        url = request.url
        spider.chrome.get(url)
        html = spider.chrome.page_source
        return HtmlResponse(url=url, body=html, requset=requset, encoding='utf-8')
```

```python
# spider
import scrapy
from scrapy import signals
from selenium import webdriver

class LoginSpider(scrapy.Spider):
    name = 'login'
    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super(LoginSpider, cls).from_crawler(crawler, *args, **kwargs)
        spider.chrome = webdriver.Chrome()
        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)
        return spider

    def spider_close(self, spider):
        spider.chrome.quit()
        print('爬虫结束了')

    def parse(self, response):
        pass
```

