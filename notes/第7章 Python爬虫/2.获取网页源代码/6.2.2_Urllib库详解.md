# Urllib库详解

---

学习爬虫，最初的操作便是把浏览器向服务器发出请求，用urllib库，不用深入深入到底层去了解它到底是怎样传输和通信。

## urllib模块

| 模块               | 功能               |
| ------------------ | ------------------ |
| urllib.request     | 请求模块           |
| urllib.error       | 异常处理模块       |
| urllib.parse       | url解析模块        |
| urllib.robotparser | robots.txt解析模块 |

## 发送请求

使用urllib的request模块，我们可以方便的实现请求的发送并得到响应。

### urlopen

urllib.request模块提供了基本的构造HTTP请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有处理授权验证码，重定向，浏览器cookies和其他内容。

```python
import urllib.request
import urllib.parse

# 发起请求
response = urllib.request.urlopen('http://httpbin.org')
print(response.read().decode('utf8'))
print(type(response))

# 获得属性
print(response.status)
print(response.getheaders())
print(response.getheader('Server'))

# data参数
data = bytes(urllib.parse.urlencode({'word':'hello'}), encoding='utf-8')
response = urllib.request.urlopen('http://httpbin.org/post', data=data)
print(response.read())

# timeout参数
response = urllib.request.urlopen('http://httpbin.org/get', timeout=1)
print(response.read())
```

### Request

```python
# 综合用法
from urllib import request, parse

url = 'http://httpbin.org/post'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',
    'Host':'httpbin.org'
}
dic = {
    'name':'Gemey'
}
data = bytes(parse.urlencode(dic), encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')
response = request.urlopen(req)
print(response.read().decode('utf8'))
```

### 高级用法

```python
# 1.验证用户名和密码
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError

username = 'username'
password = 'password'
url = 'http://localhost:5000/'

p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
openner = build_opener(auth_handler)
try:
    result = openner.open(url)
    html = result.read().decode('utf-8')
    print(html)
except URLError as e:
    print(e.reason)
```

```python
# 2.添加代理，代理实时更新，不能一直使用
import urllib.request

proxy_handler = urllib.request.ProxyHandler({
    'http':'http://127.0.0.1:9743',
    'https':'https://127.0.0.1:9743'
})
opener = urllib.request.build_opener(proxy_handler)
response = opener.open('http://www.baidu.com')
print(response.read())
```

```python
# 3.cookies
import http.cookiejar
import urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
for item in cookie:
    print(item.name+'='+item.value)
```

## 异常处理

```python
# 异常处理
from urllib import request, error

try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.URLError as e:
    print(e.reason)
```

## 解析链接

```python
# urlparse解析
from urllib.parse import urlparse

result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
print(type(result), result)
```

```python
# urlunparse进行拼接
from urllib.parse import urlunparse

data = ['http','www.baidu.com','index.html','user','a=6','comment']
print(urlunparse(data))
```

```python
# urljoin
from urllib.parse import urljoin
print(urljoin('http://www.baidu.com','https://cuiqingcai.com/FAQ.html'))
```

```python
# urlencode
from urllib.parse import urlencode

params = {
    'name':'gemey',
    'age':22
}
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)
print(url)
```

## 分析Robots协议

Robots协议也称为爬虫协议，机器人协议，它的全名叫做爬虫排除标准，用来告诉爬虫和所有引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫做robots.txt的文本文件，一般放在网站的根目录下。当搜索爬虫访问一个站点时，首先会检查这个站点检查这个根目录是否存在协议文件。useragent规定哪些爬虫不能爬，Disallow规定不允许抓取的目录。