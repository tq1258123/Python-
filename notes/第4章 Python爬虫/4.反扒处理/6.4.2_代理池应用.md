# 代理的使用

---

我们在做爬虫的过程中经常会遇到这样的情况：最初爬虫正常运行，正常之前数据，一切看起来都很好。但是有时候会报`403 Forbidden`；这时候网页上可能会出现“您的`IP`访问频率太高”这样的提示。出现这个现象的原因是网站采取了反扒措施，服务器会检测某个`IP`在单位时间内的请求次数，如果超过就会拒绝服务，这时候就需要用代理。

## 代理池简单实例

```mermaid
graph LR
	A[爬取ip] --> B[验证ip可用性]
	B --> C[保存可以用的ip]
```

这个只是网络上简单的代理池应用，正规的代理池维护还需要数据库等其他内容，暂时无法找到这方面完整内容。

```python
import requests
from fake_useragent import UserAgent
from lxml import etree
import json

# 在西刺网站上爬取ip，不要用自己的ip爬取太多次，会被封
# xpath爬取的内容是列表，要转化为字典
def get_ip(url):
    response = requests.get(url, headers={'User-Agent': UserAgent().chrome}).text
    e = etree.HTML(response)
    type_ip = e.xpath('//tr/td[6]/text()')
    ip = e.xpath('//tr/td[2]/text()')
    port = e.xpath('//tr/td[3]/text()')
    ip_list = []
    for i in range(len(type_ip)):
        ip_dict = {}
        ip_dict[type_ip[i]] = ip[i] + ':' + port[i]
        ip_list.append(ip_dict)
    return ip_list

# 用百度网站验证ip是否可用
def test_ip(ip_list):
    good_ip = []
    i = 1
    for ip in ip_list:
        print('开始测试第{}个ip'.format(i))
        test_url = 'https://www.baidu.com'
        response = requests.get(test_url, headers={'User-Agent': UserAgent().chrome}, proxies=ip, timeout=0.5)
        if response.status_code == 200:
            del ip['no']
            good_ip.append(ip)
        else:
            print('该ip不可用')
        i += 1
    return good_ip

# 保存ip
def save_ip(good_ip):
    with open('ip.txt', 'a') as f:
        for ip in good_ip:
            f.write(json.dumps(ip) + '\n')

# 主函数，爬取了2页
def main():
    for i in range(1, 3):
        url = 'https://www.xicidaili.com/nn/{}'.format(i)
        ip_list = get_ip(url)
        good_ip = test_ip(ip_list)
        save_ip(good_ip)

if __name__ == '__main__':
    main()
```

